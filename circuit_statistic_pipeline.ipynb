{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8df953b-5502-476b-81bd-018e62c391f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|circuitID| circuitRef|                name|    location|  country|     lat|    lng|alt|                 url|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|        1|albert_park|Albert Park Grand...|   Melbourne|Australia|-37.8497|144.968| 10|http://en.wikiped...|\n",
      "|        2|     sepang|Sepang Internatio...|Kuala Lumpur| Malaysia| 2.76083|101.738| 18|http://en.wikiped...|\n",
      "|        3|    bahrain|Bahrain Internati...|      Sakhir|  Bahrain| 26.0325|50.5106|  7|http://en.wikiped...|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "|driverID|driverRef|number|code|forename| surname|       dob|nationality|                 url|\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "|       1| hamilton|    44| HAM|   Lewis|Hamilton|1985-01-07|    British|http://en.wikiped...|\n",
      "|       2| heidfeld|  null| HEI|    Nick|Heidfeld|1977-05-10|     German|http://en.wikiped...|\n",
      "|       3|  rosberg|     6| ROS|    Nico| Rosberg|1985-06-27|     German|http://en.wikiped...|\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+--------+---+--------+--------+-----------+\n",
      "|raceID|driverID|lap|position|    time|miliseconds|\n",
      "+------+--------+---+--------+--------+-----------+\n",
      "|   841|      20|  1|       1|1:38.109|      98109|\n",
      "|   841|      20|  2|       1|1:33.006|      93006|\n",
      "|   841|      20|  3|       1|1:32.713|      92713|\n",
      "+------+--------+---+--------+--------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+--------+----+---+--------+--------+-----------+\n",
      "|raceID|driverID|stop|lap|    time|duration|miliseconds|\n",
      "+------+--------+----+---+--------+--------+-----------+\n",
      "|   841|     153|   1|  1|17:05:23|  26.898|      26898|\n",
      "|   841|      30|   1|  1|17:05:52|  25.021|      25021|\n",
      "|   841|      17|   1| 11|17:20:48|  23.426|      23426|\n",
      "+------+--------+----+---+--------+--------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------+--------+\n",
      "|raceID|year|round|circuitID|                name|      date|    time|                 url|fp1_date|fp1_time|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------+--------+\n",
      "|     1|2009|    1|        1|Australian Grand ...|2009-03-29|06:00:00|http://en.wikiped...|    null|      \\N|\n",
      "|     2|2009|    2|        2|Malaysian Grand Prix|2009-04-05|09:00:00|http://en.wikiped...|    null|      \\N|\n",
      "|     3|2009|    3|       17|  Chinese Grand Prix|2009-04-19|07:00:00|http://en.wikiped...|    null|      \\N|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, DoubleType, IntegerType, DateType\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Trackwide_data\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "\n",
    "# Load the circuits data\n",
    "\n",
    "# define schema\n",
    "circuit_schema = StructType([\n",
    "    StructField(\"circuitID\", StringType(), True),\n",
    "    StructField(\"circuitRef\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lng\", DoubleType(), True),\n",
    "    StructField(\"alt\", LongType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_circuits = 'gs://data-group1-ass2/circuits.csv'\n",
    "\n",
    "# Create data frame\n",
    "circuits = spark.read.format(\"csv\").schema(circuit_schema).option(\"header\", \"true\").load(gsc_file_path_circuits)\n",
    "circuits.show(3)\n",
    "\n",
    "# Load the drivers data\n",
    "\n",
    "# define schema\n",
    "drivers_schema = StructType([\n",
    "    StructField(\"driverID\", StringType(), True),\n",
    "    StructField(\"driverRef\", StringType(), True),\n",
    "    StructField(\"number\", LongType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"forename\", StringType(), True), \n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_drivers = 'gs://data-group1-ass2/drivers.csv'\n",
    "\n",
    "# Create data frame\n",
    "drivers = spark.read.format(\"csv\").schema(drivers_schema).option(\"header\", \"true\").load(gsc_file_path_drivers)\n",
    "drivers.show(3)\n",
    "\n",
    "# Load the lap_times data\n",
    "\n",
    "# define schema\n",
    "lap_times_schema = StructType([\n",
    "    StructField(\"raceID\", StringType(), True),\n",
    "    StructField(\"driverID\", StringType(), True),\n",
    "    StructField(\"lap\", LongType(), True),\n",
    "    StructField(\"position\", LongType(), True),\n",
    "    StructField(\"time\", StringType(), True), #StructField(\"time\", DayTimeIntervalType(\"MINUTE\", \"SECOND\"), True),\n",
    "    StructField(\"miliseconds\", LongType(), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_lap_times = 'gs://data-group1-ass2/lap_times.csv'\n",
    "\n",
    "# Create data frame\n",
    "lap_times = spark.read.format(\"csv\").schema(lap_times_schema).option(\"header\", \"true\").load(gsc_file_path_lap_times)\n",
    "lap_times.show(3)\n",
    "\n",
    "# Load the pit_stops data\n",
    "\n",
    "# define schema\n",
    "pit_stops_schema = StructType([\n",
    "    StructField(\"raceID\", StringType(), True),\n",
    "    StructField(\"driverID\", StringType(), True),\n",
    "    StructField(\"stop\", IntegerType(), True),\n",
    "    StructField(\"lap\", IntegerType(), True),\n",
    "    StructField(\"time\", StringType(), True), #StructField(\"time\", DayTimeIntervalType(\"MINUTE\", \"SECOND\"), True),\n",
    "    StructField(\"duration\", DoubleType(), True),\n",
    "    StructField(\"miliseconds\", LongType(), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_pit_stops = 'gs://data-group1-ass2/pit_stops.csv'\n",
    "\n",
    "# Create data frame\n",
    "pit_stops = spark.read.format(\"csv\").schema(pit_stops_schema).option(\"header\", \"true\").load(gsc_file_path_pit_stops)\n",
    "pit_stops.show(3)\n",
    "\n",
    "# Load the races data\n",
    "\n",
    "# define schema\n",
    "races_schema = StructType([\n",
    "    StructField(\"raceID\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"round\", IntegerType(), True),\n",
    "    StructField(\"circuitID\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"time\", StringType(), True), #StructField(\"time\", DayTimeIntervalType(\"MINUTE\", \"SECOND\"), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"fp1_date\", DateType(), True),\n",
    "    StructField(\"fp1_time\", StringType(), True), #StructField(\"time\", DayTimeIntervalType(\"MINUTE\", \"SECOND\"), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_races = 'gs://data-group1-ass2/races.csv'\n",
    "\n",
    "# Create data frame\n",
    "races = spark.read.format(\"csv\").schema(races_schema).option(\"header\", \"true\").load(gsc_file_path_races)\n",
    "races.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f701365-7e94-4e20-b3de-4a7e7783c816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[raceID: string, circuitID: string, date: date]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "circuits = circuits.select(\"circuitID\", \"name\")\n",
    "circuits.na.drop(\"any\", subset=[\"circuitID\"])\n",
    "drivers = drivers.select(\"driverID\", struct(\"forename\", \"surname\").alias(\"full_name\"))\n",
    "drivers.na.drop(\"any\", subset=[\"driverID\"])\n",
    "lap_times = lap_times.select(\"raceID\", \"driverID\", \"time\", \"miliseconds\")\n",
    "lap_times.na.drop(\"any\", subset=[\"raceID\", \"driverID\"])\n",
    "pit_stops = pit_stops.select(\"raceID\", \"driverID\", \"duration\")\n",
    "pit_stops.na.drop(\"any\", subset=[\"raceID\", \"driverID\"])\n",
    "races = races.select(\"raceID\", \"circuitID\", \"date\")\n",
    "races.na.drop(\"any\", subset=[\"raceID\", \"circuitID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bbbd6-9e41-4584-97e0-45497edfdd39",
   "metadata": {},
   "source": [
    "Find the fastest lap per race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4814cb0-e28e-4143-8a49-cbaf8acf9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, dense_rank,col\n",
    "from pyspark.sql import Row, Window\n",
    "\n",
    "window_find_fasest = Window.partitionBy(\"raceID\").orderBy(col(\"miliseconds\").asc())\n",
    "\n",
    "lap_times_with_rank = lap_times.withColumn(\"rank_asc_per_race\", dense_rank().over(window_find_fasest))\n",
    "lap_times_top3_per_race = lap_times_with_rank.where((col('rank_asc_per_race') == 1) | \n",
    "                                                    (col('rank_asc_per_race') == 2) | \n",
    "                                                    (col('rank_asc_per_race') == 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669eebd5-9602-4f3b-8a03-abd462855940",
   "metadata": {},
   "source": [
    "find the fastes lap per circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31a556b8-7dcd-4f1e-a8d4-41735dec0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+--------+--------+-----------+-----------------+--------------------+\n",
      "|raceID|circuitID|      date|driverID|    time|miliseconds|rank_asc_per_race|rank_asc_per_circuit|\n",
      "+------+---------+----------+--------+--------+-----------+-----------------+--------------------+\n",
      "|  1076|        1|2022-04-10|     844|1:20.260|      80260|                1|                   1|\n",
      "|  1076|        1|2022-04-10|       4|1:20.846|      80846|                2|                   2|\n",
      "|  1076|        1|2022-04-10|     844|1:20.966|      80966|                3|                   3|\n",
      "|   101|       10|2004-07-25|       8|1:13.780|      73780|                1|                   1|\n",
      "|   101|       10|2004-07-25|      30|1:13.783|      73783|                2|                   2|\n",
      "|   101|       10|2004-07-25|      30|1:13.864|      73864|                3|                   3|\n",
      "|   136|       11|2002-08-18|      30|1:16.207|      76207|                1|                   1|\n",
      "|  1033|       11|2020-07-19|       1|1:16.627|      76627|                1|                   2|\n",
      "|   153|       11|2001-08-19|      57|1:16.723|      76723|                1|                   3|\n",
      "|    11|       12|2009-08-23|      10|1:38.683|      98683|                1|                   1|\n",
      "|    29|       12|2008-08-24|      13|1:38.708|      98708|                1|                   2|\n",
      "|    29|       12|2008-08-24|      13|1:38.757|      98757|                2|                   3|\n",
      "|   103|       13|2004-08-29|       8|1:45.108|     105108|                1|                   1|\n",
      "|   103|       13|2004-08-29|       8|1:45.351|     105351|                2|                   2|\n",
      "|   103|       13|2004-08-29|      30|1:45.503|     105503|                3|                   3|\n",
      "|   104|       14|2004-09-12|      22|1:21.046|      81046|                1|                   1|\n",
      "|   104|       14|2004-09-12|      22|1:21.279|      81279|                2|                   2|\n",
      "|   104|       14|2004-09-12|      30|1:21.361|      81361|                3|                   3|\n",
      "|  1003|       15|2018-09-16|     825|1:41.905|     101905|                1|                   1|\n",
      "|  1024|       15|2019-09-22|     825|1:42.301|     102301|                1|                   2|\n",
      "|  1003|       15|2018-09-16|       1|1:42.913|     102913|                2|                   3|\n",
      "|    33|       16|2008-10-12|      13|1:18.426|      78426|                1|                   1|\n",
      "|    33|       16|2008-10-12|      13|1:18.724|      78724|                2|                   2|\n",
      "|    33|       16|2008-10-12|      13|1:18.753|      78753|                3|                   3|\n",
      "|   105|       17|2004-09-26|      30|1:32.238|      92238|                1|                   1|\n",
      "|   105|       17|2004-09-26|      22|1:32.455|      92455|                2|                   2|\n",
      "|   105|       17|2004-09-26|      30|1:32.576|      92576|                3|                   3|\n",
      "|  1008|       18|2018-11-11|     822|1:10.540|      70540|                1|                   1|\n",
      "|  1029|       18|2019-11-17|     822|1:10.698|      70698|                1|                   2|\n",
      "|  1008|       18|2018-11-11|      20|1:10.831|      70831|                2|                   3|\n",
      "|    98|       19|2004-06-20|      22|1:10.399|      70399|                1|                   1|\n",
      "|    98|       19|2004-06-20|      30|1:10.412|      70412|                2|                   2|\n",
      "|    98|       19|2004-06-20|      30|1:10.434|      70434|                3|                   3|\n",
      "|   983|        2|2017-10-01|      20|1:34.080|      94080|                1|                   1|\n",
      "|   983|        2|2017-10-01|      20|1:34.218|      94218|                2|                   2|\n",
      "|   983|        2|2017-10-01|      20|1:34.222|      94222|                3|                   3|\n",
      "|   149|       20|2001-06-24|      31|1:18.354|      78354|                1|                   1|\n",
      "|   149|       20|2001-06-24|      31|1:18.427|      78427|                2|                   2|\n",
      "|   149|       20|2001-06-24|      23|1:18.498|      78498|                3|                   3|\n",
      "|  1043|       21|2020-11-01|       1|1:15.484|      75484|                1|                   1|\n",
      "|  1043|       21|2020-11-01|     822|1:15.902|      75902|                2|                   2|\n",
      "|  1043|       21|2020-11-01|       1|1:15.914|      75914|                3|                   3|\n",
      "|  1026|       22|2019-10-13|       1|1:30.983|      90983|                1|                   1|\n",
      "|  1026|       22|2019-10-13|       1|1:31.538|      91538|                2|                   2|\n",
      "|    88|       22|2005-10-09|       8|1:31.540|      91540|                1|                   3|\n",
      "|  1073|       24|2021-12-12|     830|1:26.103|      86103|                1|                   1|\n",
      "|  1073|       24|2021-12-12|     830|1:26.301|      86301|                2|                   2|\n",
      "|  1073|       24|2021-12-12|     830|1:26.321|      86321|                3|                   3|\n",
      "|   209|       25|1997-04-13|      77|1:27.981|      87981|                1|                   1|\n",
      "|   209|       25|1997-04-13|      77|1:28.000|      88000|                2|                   2|\n",
      "+------+---------+----------+--------+--------+-----------+-----------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "races_with_top_3_laps = races.join(lap_times_top3_per_race, ['raceID'])\n",
    "\n",
    "window_find_fasest_circuit = Window.partitionBy(\"circuitID\").orderBy(col(\"miliseconds\").asc())\n",
    "\n",
    "races_with_lap_rank = races_with_top_3_laps.withColumn(\"rank_asc_per_circuit\", dense_rank().over(window_find_fasest_circuit))\n",
    "lap_times_top3_per_circuit = races_with_lap_rank.where((col('rank_asc_per_circuit') == 1) | \n",
    "                                                    (col('rank_asc_per_circuit') == 2) | \n",
    "                                                    (col('rank_asc_per_circuit') == 3))\n",
    "\n",
    "lap_times_top3_per_circuit.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244729d-8e1f-4a7f-b11b-fecbe62b8942",
   "metadata": {},
   "source": [
    "add driver name and circuit name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60bf4bf7-1d4f-420d-bcb0-1d82351fa81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+------------+---------------------+-----------------------+-------------------+--------------------+----------------------+------------------+\n",
      "|        circuit_name|      fastest_driver|fastest_lap_time|date_fastest|second_fastest_driver|second_fastest_lap_time|date_second_fastest|third_fastest_driver|third_fastest_lap_time|date_third_fastest|\n",
      "+--------------------+--------------------+----------------+------------+---------------------+-----------------------+-------------------+--------------------+----------------------+------------------+\n",
      "|Albert Park Grand...|  {Charles, Leclerc}|        1:20.260|  2022-04-10|   {Fernando, Alonso}|               1:20.846|         2022-04-10|  {Charles, Leclerc}|              1:20.966|        2022-04-10|\n",
      "|Autodromo Enzo e ...|   {Lewis, Hamilton}|        1:15.484|  2020-11-01|   {Valtteri, Bottas}|               1:15.902|         2020-11-01|   {Lewis, Hamilton}|              1:15.914|        2020-11-01|\n",
      "|Autodromo Interna...|   {Lewis, Hamilton}|        1:18.833|  2020-09-13|    {Lewis, Hamilton}|               1:19.338|         2020-09-13|  {Valtteri, Bottas}|              1:19.432|        2020-09-13|\n",
      "|Autodromo Naziona...|{Rubens, Barriche...|        1:21.046|  2004-09-12| {Rubens, Barriche...|               1:21.279|         2004-09-12|{Michael, Schumac...|              1:21.361|        2004-09-12|\n",
      "|Autódromo Hermano...|  {Valtteri, Bottas}|        1:17.774|  2021-11-07|   {Valtteri, Bottas}|               1:18.741|         2018-10-28| {Sebastian, Vettel}|              1:18.785|        2017-10-29|\n",
      "|Autódromo Interna...|   {Lewis, Hamilton}|        1:18.750|  2020-10-25|    {Lewis, Hamilton}|               1:18.949|         2020-10-25|   {Lewis, Hamilton}|              1:18.966|        2020-10-25|\n",
      "|Autódromo José Ca...|  {Valtteri, Bottas}|        1:10.540|  2018-11-11|   {Valtteri, Bottas}|               1:10.698|         2019-11-17| {Sebastian, Vettel}|              1:10.831|        2018-11-11|\n",
      "|Autódromo Juan y ...|   {Gerhard, Berger}|        1:27.981|  1997-04-13|    {Gerhard, Berger}|               1:28.000|         1997-04-13|{Jacques, Villene...|              1:28.028|        1997-04-13|\n",
      "|Autódromo do Estoril|{Jacques, Villene...|        1:22.873|  1996-09-22| {Jacques, Villene...|               1:23.378|         1996-09-22|{Jacques, Villene...|              1:23.397|        1996-09-22|\n",
      "|Bahrain Internati...|   {George, Russell}|        0:55.404|  2020-12-06|    {George, Russell}|               0:56.319|         2020-12-06|   {George, Russell}|              0:56.393|        2020-12-06|\n",
      "|   Baku City Circuit|  {Charles, Leclerc}|        1:43.009|  2019-04-28|  {Sebastian, Vettel}|               1:43.441|         2017-06-25|   {Lewis, Hamilton}|              1:43.469|        2017-06-25|\n",
      "|Buddh Internation...| {Sebastian, Vettel}|        1:27.249|  2011-10-30|  {Sebastian, Vettel}|               1:27.457|         2011-10-30|      {Mark, Webber}|              1:27.520|        2011-10-30|\n",
      "|Circuit Gilles Vi...|  {Valtteri, Bottas}|        1:13.078|  2019-06-09| {Rubens, Barriche...|               1:13.622|         2004-06-13|{Michael, Schumac...|              1:13.630|        2004-06-13|\n",
      "|Circuit Park Zand...|   {Lewis, Hamilton}|        1:11.097|  2021-09-05|   {Valtteri, Bottas}|               1:12.549|         2021-09-05|   {Lewis, Hamilton}|              1:13.124|        2021-09-05|\n",
      "| Circuit Paul Ricard| {Sebastian, Vettel}|        1:32.740|  2019-06-23|    {Lewis, Hamilton}|               1:32.764|         2019-06-23|  {Valtteri, Bottas}|              1:33.586|        2019-06-23|\n",
      "|Circuit de Barcel...|{Giancarlo, Fisic...|        1:15.641|  2005-05-08| {Michael, Schumac...|               1:15.648|         2005-05-08|{Giancarlo, Fisic...|              1:15.741|        2005-05-08|\n",
      "|   Circuit de Monaco|   {Lewis, Hamilton}|        1:12.909|  2021-05-23|    {Lewis, Hamilton}|               1:13.382|         2021-05-23|     {Yuki, Tsunoda}|              1:14.037|        2021-05-23|\n",
      "|Circuit de Nevers...|  {David, Coulthard}|        1:15.045|  2002-07-21|   {David, Coulthard}|               1:15.052|         2002-07-21|  {David, Coulthard}|              1:15.159|        2002-07-21|\n",
      "|Circuit de Spa-Fr...|   {Kimi, Räikkönen}|        1:45.108|  2004-08-29|    {Kimi, Räikkönen}|               1:45.351|         2004-08-29|{Michael, Schumac...|              1:45.503|        2004-08-29|\n",
      "|Circuit of the Am...|  {Charles, Leclerc}|        1:36.169|  2019-11-03|   {Valtteri, Bottas}|               1:36.957|         2019-11-03|   {Lewis, Hamilton}|              1:37.392|        2018-10-21|\n",
      "|   Circuito de Jerez|{Heinz-Harald, Fr...|        1:23.135|  1997-10-26|    {Gerhard, Berger}|               1:23.361|         1997-10-26|{Michael, Schumac...|              1:23.692|        1997-10-26|\n",
      "|       Fuji Speedway|     {Felipe, Massa}|        1:18.426|  2008-10-12|      {Felipe, Massa}|               1:18.724|         2008-10-12|     {Felipe, Massa}|              1:18.753|        2008-10-12|\n",
      "|      Hockenheimring|   {Kimi, Räikkönen}|        1:13.780|  2004-07-25| {Michael, Schumac...|               1:13.783|         2004-07-25|{Michael, Schumac...|              1:13.864|        2004-07-25|\n",
      "|         Hungaroring|{Michael, Schumac...|        1:16.207|  2002-08-18|    {Lewis, Hamilton}|               1:16.627|         2020-07-19|    {Mika, Häkkinen}|              1:16.723|        2001-08-19|\n",
      "|Indianapolis Moto...|{Rubens, Barriche...|        1:10.399|  2004-06-20| {Michael, Schumac...|               1:10.412|         2004-06-20|{Michael, Schumac...|              1:10.434|        2004-06-20|\n",
      "|       Istanbul Park|{Juan, Pablo Mont...|        1:24.770|  2005-08-21| {Juan, Pablo Mont...|               1:24.997|         2005-08-21|   {Kimi, Räikkönen}|              1:25.030|        2005-08-21|\n",
      "|Jeddah Corniche C...|   {Lewis, Hamilton}|        1:30.734|  2021-12-05|    {Lewis, Hamilton}|               1:30.854|         2021-12-05|   {Lewis, Hamilton}|              1:31.089|        2021-12-05|\n",
      "|Korean Internatio...| {Sebastian, Vettel}|        1:39.605|  2011-10-16|       {Mark, Webber}|               1:40.294|         2011-10-16|   {Lewis, Hamilton}|              1:40.459|        2011-10-16|\n",
      "|Losail Internatio...|   {Max, Verstappen}|        1:23.196|  2021-11-21|    {Max, Verstappen}|               1:24.031|         2021-11-21|   {Max, Verstappen}|              1:25.030|        2021-11-21|\n",
      "|Marina Bay Street...|  {Kevin, Magnussen}|        1:41.905|  2018-09-16|   {Kevin, Magnussen}|               1:42.301|         2019-09-22|   {Lewis, Hamilton}|              1:42.913|        2018-09-16|\n",
      "|Miami Internation...|   {Max, Verstappen}|        1:31.361|  2022-05-08|    {Max, Verstappen}|               1:31.456|         2022-05-08|   {Max, Verstappen}|              1:31.458|        2022-05-08|\n",
      "|         Nürburgring|{Juan, Pablo Mont...|        1:18.354|  2001-06-24| {Juan, Pablo Mont...|               1:18.427|         2001-06-24|  {Ralf, Schumacher}|              1:18.498|        2001-06-24|\n",
      "|       Red Bull Ring|     {Carlos, Sainz}|        1:05.619|  2020-07-12|    {Max, Verstappen}|               1:06.145|         2020-07-12|   {Max, Verstappen}|              1:06.200|        2021-07-04|\n",
      "|Sepang Internatio...| {Sebastian, Vettel}|        1:34.080|  2017-10-01|  {Sebastian, Vettel}|               1:34.218|         2017-10-01| {Sebastian, Vettel}|              1:34.222|        2017-10-01|\n",
      "|Shanghai Internat...|{Michael, Schumac...|        1:32.238|  2004-09-26| {Rubens, Barriche...|               1:32.455|         2004-09-26|{Michael, Schumac...|              1:32.576|        2004-09-26|\n",
      "| Silverstone Circuit|{Michael, Schumac...|        1:18.739|  2004-07-11| {Michael, Schumac...|               1:18.918|         2004-07-11|{Michael, Schumac...|              1:19.223|        2004-07-11|\n",
      "|      Sochi Autodrom|   {Lewis, Hamilton}|        1:35.761|  2019-09-29|   {Valtteri, Bottas}|               1:35.861|         2018-09-30|  {Valtteri, Bottas}|              1:35.887|        2018-09-30|\n",
      "|      Suzuka Circuit|   {Lewis, Hamilton}|        1:30.983|  2019-10-13|    {Lewis, Hamilton}|               1:31.538|         2019-10-13|   {Kimi, Räikkönen}|              1:31.540|        2005-10-09|\n",
      "|Valencia Street C...|       {Timo, Glock}|        1:38.683|  2009-08-23|      {Felipe, Massa}|               1:38.708|         2008-08-24|     {Felipe, Massa}|              1:38.757|        2008-08-24|\n",
      "|  Yas Marina Circuit|   {Max, Verstappen}|        1:26.103|  2021-12-12|    {Max, Verstappen}|               1:26.301|         2021-12-12|   {Max, Verstappen}|              1:26.321|        2021-12-12|\n",
      "+--------------------+--------------------+----------------+------------+---------------------+-----------------------+-------------------+--------------------+----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "fastest_laps_combined_data = lap_times_top3_per_circuit.join(drivers, ['driverID']).join(circuits, ['circuitID'])\n",
    "\n",
    "first = fastest_laps_combined_data.where('rank_asc_per_circuit == 1').selectExpr(\"name as circuit_name\", \n",
    "                                                                                 \"full_name as fastest_driver\",\n",
    "                                                                                 \"time as fastest_lap_time\",\n",
    "                                                                                 \"date as date_fastest\")\n",
    "second = fastest_laps_combined_data.where('rank_asc_per_circuit == 2').selectExpr(\"name as circuit_name\", \n",
    "                                                                                 \"full_name as second_fastest_driver\",\n",
    "                                                                                 \"time as second_fastest_lap_time\",\n",
    "                                                                                 \"date as date_second_fastest\")\n",
    "third = fastest_laps_combined_data.where('rank_asc_per_circuit == 3').selectExpr(\"name as circuit_name\", \n",
    "                                                                                 \"full_name as third_fastest_driver\",\n",
    "                                                                                 \"time as third_fastest_lap_time\",\n",
    "                                                                                 \"date as date_third_fastest\")\n",
    "\n",
    "fastest_lap_final_data = first.join(second, ['circuit_name']).join(third, ['circuit_name'])\n",
    "\n",
    "fastest_lap_final_data.sort('circuit_name').show(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da13958e-574c-4a7e-9a4a-ff2c8ecc448b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o679.save.\n: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Request had insufficient authentication scopes.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:778)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:775)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:774)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)\n\tat com.google.cloud.spark.bigquery.write.BigQueryInsertableRelationBase.<init>(BigQueryInsertableRelationBase.java:47)\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.<init>(BigQueryDeprecatedIndirectInsertableRelation.java:33)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelationInternal(CreatableRelationProviderHelper.java:119)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelation(CreatableRelationProviderHelper.java:95)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:47)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://www.googleapis.com/bigquery/v2/projects/deassignment2/datasets/Output_processing_pipeline/tables/fastest_laps?prettyPrint=false\n{\n  \"code\" : 403,\n  \"details\" : [ {\n    \"@type\" : \"type.googleapis.com/google.rpc.ErrorInfo\",\n    \"reason\" : \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\n  } ],\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Insufficient Permission\",\n    \"reason\" : \"insufficientPermissions\"\n  } ],\n  \"message\" : \"Request had insufficient authentication scopes.\",\n  \"status\" : \"PERMISSION_DENIED\"\n}\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)\n\t... 54 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporaryGcsBucket\u001b[39m\u001b[38;5;124m'\u001b[39m, bucket)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Saving the data to BigQuery\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[43mfastest_lap_final_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcircuit_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeassignment2.Output_processing_pipeline.fastest_laps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:966\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o679.save.\n: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Request had insufficient authentication scopes.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:299)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:778)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$18.call(BigQueryImpl.java:775)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.run(RetryHelper.java:76)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.getTable(BigQueryImpl.java:774)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.getTable(BigQueryClient.java:121)\n\tat com.google.cloud.spark.bigquery.write.BigQueryInsertableRelationBase.<init>(BigQueryInsertableRelationBase.java:47)\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.<init>(BigQueryDeprecatedIndirectInsertableRelation.java:33)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelationInternal(CreatableRelationProviderHelper.java:119)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelation(CreatableRelationProviderHelper.java:95)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:47)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:106)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://www.googleapis.com/bigquery/v2/projects/deassignment2/datasets/Output_processing_pipeline/tables/fastest_laps?prettyPrint=false\n{\n  \"code\" : 403,\n  \"details\" : [ {\n    \"@type\" : \"type.googleapis.com/google.rpc.ErrorInfo\",\n    \"reason\" : \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\n  } ],\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"Insufficient Permission\",\n    \"reason\" : \"insufficientPermissions\"\n  } ],\n  \"message\" : \"Request had insufficient authentication scopes.\",\n  \"status\" : \"PERMISSION_DENIED\"\n}\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:576)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.getTable(HttpBigQueryRpc.java:297)\n\t... 54 more\n"
     ]
    }
   ],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp_group1_ass2\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "fastest_lap_final_data.sort('circuit_name').write.format('bigquery') \\\n",
    "  .option('table', 'deassignment2.Output_processing_pipeline.fastest_laps') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2dd69b-be53-430c-beca-1f2ccf1d966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
