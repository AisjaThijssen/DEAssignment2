{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8df953b-5502-476b-81bd-018e62c391f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|circuitID| circuitRef|                name|    location|  country|     lat|    lng|alt|                 url|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|        1|albert_park|Albert Park Grand...|   Melbourne|Australia|-37.8497|144.968| 10|http://en.wikiped...|\n",
      "|        2|     sepang|Sepang Internatio...|Kuala Lumpur| Malaysia| 2.76083|101.738| 18|http://en.wikiped...|\n",
      "|        3|    bahrain|Bahrain Internati...|      Sakhir|  Bahrain| 26.0325|50.5106|  7|http://en.wikiped...|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "|driverID|driverRef|number|code|forename| surname|       dob|nationality|                 url|\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "|       1| hamilton|    44| HAM|   Lewis|Hamilton|1985-01-07|    British|http://en.wikiped...|\n",
      "|       2| heidfeld|  null| HEI|    Nick|Heidfeld|1977-05-10|     German|http://en.wikiped...|\n",
      "|       3|  rosberg|     6| ROS|    Nico| Rosberg|1985-06-27|     German|http://en.wikiped...|\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+--------+---+--------+--------+-----------+\n",
      "|raceID|driverID|lap|position|    time|miliseconds|\n",
      "+------+--------+---+--------+--------+-----------+\n",
      "|   841|      20|  1|       1|1:38.109|      98109|\n",
      "|   841|      20|  2|       1|1:33.006|      93006|\n",
      "|   841|      20|  3|       1|1:32.713|      92713|\n",
      "+------+--------+---+--------+--------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------+--------+\n",
      "|raceID|year|round|circuitID|                name|      date|    time|                 url|fp1_date|fp1_time|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------+--------+\n",
      "|     1|2009|    1|        1|Australian Grand ...|2009-03-29|06:00:00|http://en.wikiped...|    null|      \\N|\n",
      "|     2|2009|    2|        2|Malaysian Grand Prix|2009-04-05|09:00:00|http://en.wikiped...|    null|      \\N|\n",
      "|     3|2009|    3|       17|  Chinese Grand Prix|2009-04-19|07:00:00|http://en.wikiped...|    null|      \\N|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, DoubleType, IntegerType, DateType\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Circuits_pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Load the circuits data\n",
    "\n",
    "# define schema\n",
    "circuit_schema = StructType([\n",
    "    StructField(\"circuitID\", StringType(), True),\n",
    "    StructField(\"circuitRef\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lng\", DoubleType(), True),\n",
    "    StructField(\"alt\", LongType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_circuits = 'gs://data-group1-ass2/circuits.csv'\n",
    "\n",
    "# Create data frame\n",
    "circuits = spark.read.format(\"csv\").schema(circuit_schema).option(\"header\", \"true\").load(gsc_file_path_circuits)\n",
    "circuits.show(3)\n",
    "\n",
    "# Load the drivers data\n",
    "\n",
    "# define schema\n",
    "drivers_schema = StructType([\n",
    "    StructField(\"driverID\", StringType(), True),\n",
    "    StructField(\"driverRef\", StringType(), True),\n",
    "    StructField(\"number\", LongType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"forename\", StringType(), True), \n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_drivers = 'gs://data-group1-ass2/drivers.csv'\n",
    "\n",
    "# Create data frame\n",
    "drivers = spark.read.format(\"csv\").schema(drivers_schema).option(\"header\", \"true\").load(gsc_file_path_drivers)\n",
    "drivers.show(3)\n",
    "\n",
    "# Load the lap_times data\n",
    "\n",
    "# define schema\n",
    "lap_times_schema = StructType([\n",
    "    StructField(\"raceID\", StringType(), True),\n",
    "    StructField(\"driverID\", StringType(), True),\n",
    "    StructField(\"lap\", LongType(), True),\n",
    "    StructField(\"position\", LongType(), True),\n",
    "    StructField(\"time\", StringType(), True), #StructField(\"time\", DayTimeIntervalType(\"MINUTE\", \"SECOND\"), True),\n",
    "    StructField(\"miliseconds\", LongType(), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_lap_times = 'gs://data-group1-ass2/lap_times.csv'\n",
    "\n",
    "# Create data frame\n",
    "lap_times = spark.read.format(\"csv\").schema(lap_times_schema).option(\"header\", \"true\").load(gsc_file_path_lap_times)\n",
    "lap_times.show(3)\n",
    "\n",
    "# Load the races data\n",
    "\n",
    "# define schema\n",
    "races_schema = StructType([\n",
    "    StructField(\"raceID\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"round\", IntegerType(), True),\n",
    "    StructField(\"circuitID\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"time\", StringType(), True), #StructField(\"time\", DayTimeIntervalType(\"MINUTE\", \"SECOND\"), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"fp1_date\", DateType(), True),\n",
    "    StructField(\"fp1_time\", StringType(), True), #StructField(\"time\", DayTimeIntervalType(\"MINUTE\", \"SECOND\"), True),\n",
    "])\n",
    "\n",
    "gsc_file_path_races = 'gs://data-group1-ass2/races.csv'\n",
    "\n",
    "# Create data frame\n",
    "races = spark.read.format(\"csv\").schema(races_schema).option(\"header\", \"true\").load(gsc_file_path_races)\n",
    "races.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f701365-7e94-4e20-b3de-4a7e7783c816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[raceID: string, circuitID: string, date: date]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "circuits = circuits.select(\"circuitID\", \"name\")\n",
    "circuits.na.drop(\"any\", subset=[\"circuitID\"])\n",
    "drivers = drivers.select(\"driverID\", concat(\"forename\",lit(\" \"),  \"surname\").alias(\"full_name\"))\n",
    "drivers.na.drop(\"any\", subset=[\"driverID\"])\n",
    "lap_times = lap_times.select(\"raceID\", \"driverID\", \"time\", \"miliseconds\")\n",
    "lap_times.na.drop(\"any\", subset=[\"raceID\", \"driverID\"])\n",
    "races = races.select(\"raceID\", \"circuitID\", \"date\")\n",
    "races.na.drop(\"any\", subset=[\"raceID\", \"circuitID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bbbd6-9e41-4584-97e0-45497edfdd39",
   "metadata": {},
   "source": [
    "Find the fastest lap per race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4814cb0-e28e-4143-8a49-cbaf8acf9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, dense_rank,col\n",
    "from pyspark.sql import Row, Window\n",
    "\n",
    "window_find_fasest = Window.partitionBy(\"raceID\").orderBy(col(\"miliseconds\").asc())\n",
    "\n",
    "lap_times_with_rank = lap_times.withColumn(\"rank_asc_per_race\", dense_rank().over(window_find_fasest))\n",
    "lap_times_top3_per_race = lap_times_with_rank.where((col('rank_asc_per_race') == 1) | \n",
    "                                                    (col('rank_asc_per_race') == 2) | \n",
    "                                                    (col('rank_asc_per_race') == 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669eebd5-9602-4f3b-8a03-abd462855940",
   "metadata": {},
   "source": [
    "find the fastes lap per circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a556b8-7dcd-4f1e-a8d4-41735dec0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "races_with_top_3_laps = races.join(lap_times_top3_per_race, ['raceID'])\n",
    "\n",
    "window_find_fasest_circuit = Window.partitionBy(\"circuitID\").orderBy(col(\"miliseconds\").asc())\n",
    "\n",
    "races_with_lap_rank = races_with_top_3_laps.withColumn(\"rank_asc_per_circuit\", dense_rank().over(window_find_fasest_circuit))\n",
    "lap_times_top3_per_circuit = races_with_lap_rank.where((col('rank_asc_per_circuit') == 1) | \n",
    "                                                       (col('rank_asc_per_circuit') == 2) | \n",
    "                                                       (col('rank_asc_per_circuit') == 3))\n",
    "\n",
    "lap_times_top3_per_circuit.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244729d-8e1f-4a7f-b11b-fecbe62b8942",
   "metadata": {},
   "source": [
    "add driver name and circuit name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf4bf7-1d4f-420d-bcb0-1d82351fa81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "fastest_laps_combined_data = lap_times_top3_per_circuit.join(drivers, ['driverID']).join(circuits, ['circuitID'])\n",
    "\n",
    "fastest_lap_final_data = fastest_laps_combined_data.selectExpr(\"name as circuit_name\",\n",
    "                                                               \"rank_asc_per_circuit as rank\",\n",
    "                                                               \"full_name\",\n",
    "                                                               \"time as lap_time\",\n",
    "                                                               \"date as race_date\",)\n",
    "fastest_lap_final_data.sort('circuit_name', 'rank').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13958e-574c-4a7e-9a4a-ff2c8ecc448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp-group1-ass2\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "fastest_lap_final_data.sort('circuit_name').write.format('bigquery').option('table', 'deassignment2.Output_processing_pipeline.fastest_laps')\\\n",
    "    .mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2dd69b-be53-430c-beca-1f2ccf1d966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
